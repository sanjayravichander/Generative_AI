import streamlit as st
import os
import time
from dotenv import load_dotenv
from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain_community.vectorstores import FAISS

# Load API Key securely
api_key = st.secrets["NVIDIA_API_KEY"]

# Initialize Streamlit app
st.set_page_config(page_title="RAG App with NVIDIA NIM", layout="wide")
st.title("üìò RAG-based AI Assistant using NVIDIA NIM")

# NVIDIA Model
llm = ChatNVIDIA(model="deepseek-ai/deepseek-r1-distill-llama-8b", nvidia_api_key=api_key, temperature=0.56)

# Session state initialization
if "vectors" not in st.session_state:
    st.session_state.vectors = None
    st.session_state.processed = False

# File Uploader
uploaded_files = st.file_uploader("üìÇ Upload PDFs (Multiple Allowed)", type=["pdf"], accept_multiple_files=True)

# Embedding function
def vector_embedding(files):
    if not files:
        st.warning("‚ö†Ô∏è Please upload PDFs before embedding.")
        return
    
    st.session_state.embeddings = NVIDIAEmbeddings()
    docs = []

    for file in files:
        with open(file.name, "wb") as f:
            f.write(file.getbuffer())
        loader = PyPDFLoader(file.name)
        docs.extend(loader.load())

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    final_documents = text_splitter.split_documents(docs)
    st.session_state.vectors = FAISS.from_documents(final_documents, st.session_state.embeddings)
    st.session_state.processed = True
    st.success("‚úÖ PDF Embedding Completed!")

# Embedding button
if st.button("üìå Process PDFs & Create Embeddings"):
    vector_embedding(uploaded_files)

# Prompt Input
prompt_1 = st.text_input("üîé Enter your question:")

# Prompt Template
prompt_template = ChatPromptTemplate.from_template("""
Answer the following question based on the provided context.
Ensure the response is accurate and relevant.

<context>
{context}
</context>

Question: {input}
""")

# Generating Response
if prompt_1:
    if not st.session_state.processed:
        st.warning("‚ö†Ô∏è Please upload and embed PDFs first!")
    else:
        retriever = st.session_state.vectors.as_retriever(search_type="similarity", search_kwargs={"k": 3})
        retrieved_docs = retriever.get_relevant_documents(prompt_1)

        document_chain = create_stuff_documents_chain(llm, prompt_template)
        retrieval_chain = create_retrieval_chain(retriever, document_chain)

        st.write("‚è≥ Generating response... Please wait.")
        start_time = time.process_time()
        response = retrieval_chain.invoke({"input": prompt_1})
        elapsed_time = time.process_time() - start_time

        # Display AI Response first
        if "answer" in response:
            st.subheader("ü§ñ AI Response:")
            st.write(response["answer"])
        else:
            st.error("‚ö†Ô∏è No response generated by the model. Check logs for details.")

        # Show relevant document chunks
        with st.expander("üìå Relevant Document Chunks"):
            for i, doc in enumerate(retrieved_docs):
                st.write(f"**üìú Chunk {i+1}:**")
                st.write(doc.page_content)
                st.write("------------------------------------------------")
